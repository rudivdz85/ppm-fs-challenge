Scalability Strategy for 100,000 Concurrent Users
Database Indexing:
To handle 100K concurrent users efficiently, I would implement strategic indexes on the most-queried columns. The hierarchy_structures table would have a btree index on the path column to enable fast descendant lookups using LIKE queries, and an index on parent_id for ancestor queries. The permissions table would have composite indexes on (user_id, structure_id) to optimize permission checks, and the users table would have an index on structure_id for joining with hierarchy data. These indexes transform O(n) table scans into O(log n) lookups, reducing query times from hundreds of milliseconds to single-digit milliseconds at scale.
Query Optimization:
The core permission query would be optimized using a single JOIN operation rather than N+1 queries, combining users, structures, and permissions in one database roundtrip. I'd implement cursor-based pagination instead of offset-based to maintain performance on large result sets, and use SELECT projection to only fetch required columns rather than SELECT *. Connection pooling would be configured to handle thousands of concurrent database connections efficiently without exhausting resources.
Caching Strategy:
I'd implement a multi-layer caching approach. Redis would cache the hierarchy structure (updated infrequently, queried constantly) with a 1-hour TTL, user permissions with a 15-minute TTL, and query results with a 5-minute TTL. This would reduce database load by 80-90% since the same permission checks and hierarchy traversals happen repeatedly. HTTP response caching with appropriate Cache-Control headers would enable client-side and CDN caching. Cache invalidation would be event-drivenâ€”when permissions change, we'd clear affected cache keys immediately.
Deployment Architecture:
The application would be horizontally scaled with 5-10 API server instances behind a load balancer (AWS ALB or NGINX), capable of handling 10K concurrent connections each. The database would use a primary-replica setup with read replicas handling the majority of queries (95% of traffic is reads), while the primary handles writes. Redis would be deployed as a cluster with replicas for high availability. The frontend would be served from a CDN (CloudFlare or CloudFront) for global low-latency access. This architecture allows independent scaling of each layer based on demand.
Performance Monitoring:
I'd implement APM (Application Performance Monitoring) using tools like DataDog or New Relic to track key metrics: P95 response times (target <300ms), P99 response times (target <500ms), throughput (target 1000+ requests/second), and error rates (target <0.1%). Database monitoring would track slow queries (>100ms), connection pool utilization, and cache hit ratios (target >95%). Infrastructure metrics would monitor CPU, memory, and network utilization with alerts configured to trigger auto-scaling at 70% CPU. Custom business metrics would track permission query patterns to identify hot spots and optimization opportunities. Load testing with tools like Artillery would validate the system can sustain 100K concurrent users before production deployment.